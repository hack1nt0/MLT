{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习基石\n",
    "这篇文章是对台大林轩田老师的同名课程（Machine Learnig Foundation, MLF）的总结笔记，主要分四个大部分：什么时候可以用ML、为什么ML可以work、怎么样用ML、怎么把ML用的更好，或者叫围绕这四个问题来展开叙述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. motivation\n",
    "\n",
    "    复杂系统的特点：\n",
    "+ 规则太多人工很难穷举（火星探测器的自动驾驶）\n",
    "+ 规则很难提取（语音、视觉）\n",
    "+ 人工系统不能实时学习、快速决策（高频交易）\n",
    "+ 子系统非常多，人工维护难（基于用户的推荐系统等）。\n",
    "\n",
    "    如果机器可以自己学习，包括学习特征和组合方法，上面的复杂系统就变的可行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. ML的可行性\n",
    "\n",
    "    如果一个问题本身满足三个条件：\n",
    "+ 确实有一些隐含的模式（规则）\n",
    "+ 人工系统实现这个问题比较困难（参照上面复杂系统的几个特点）\n",
    "+ 有一些蕴含这些规则的数据。\n",
    "\n",
    "那么她就是ML的用武之地（When）了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. ML的成功应用\n",
    "\n",
    "+ 衣食住行\n",
    "    + 衣：根据用户调查和销售数据，为用户推荐时装搭配\n",
    "    + 食：根据推特上的状态和位置，为用户提供餐馆的水平指数\n",
    "    + 住：根据建筑的结构和她的能耗的历史，推断其它建筑的的能耗\n",
    "    + 行：根据交通信号的图片和意义，实现自动驾驶 \n",
    "    \n",
    "    可以看出ML无处不在。    \n",
    "+ 寓教于乐\n",
    "  + 在线答题系统（教）：根据学生的实时能力选择相应难度的题目，以不断提高学生能力（从学生的答题情况记录逆向推导出学生的能力和题目的难度）\n",
    "  + 用户推荐系统（乐）：Netflix：0.1B rattings, 0.48M users, 17K movies; Yahoo!: 0.25B, 1M, 0.6M songs） \n",
    "    \n",
    "    这两个问题都可以用推荐系统（协同过滤）的思路建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV. ML和其它三个名词的关系\n",
    "+ ML: 利用数据，推断一个未知目标函数（f），得到一个近似函数g\n",
    "+ Data Mining: 挖掘数据本身的一些有趣属性，其中也包括挖掘出一个g，其近似于f；ML和DM相辅相成，很难区分\n",
    "+ Artificial Intelligence: 得到智能的东西，其通过计算的方式实现智能。ML是AI目的的一条途径。AI还有一些传统的途径\n",
    "+ Statistics: 对未知量（unknown process）进行推断（inference），这与ML的目的不约而同。传统的统计更注重理论，不太注重实现（计算量）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V. 学习问题的种类\n",
    "+ 根据Y的定义域：\n",
    "    + 二项、多项分类，回归\n",
    "    + 结构学习（Structured Learning）：很大很大的多项分类问题，其Y的域是许多小的多项分类问题输出y_i的（乘法）组合（并且整体满足一定限制条件），这种限制条件下的组合就叫原问题的结构。比如序列标注等。\n",
    "\n",
    "+ 根据有没有Y：\n",
    "    + 有监督（Supervised）：分类和回归\n",
    "    + 无监督（Unsupervised）：聚类、密度分析（无监督回归）、异常点检测（无监督分类）\n",
    "    + 半监督（Semi-supervised）：减少标注成本\n",
    "    + 强化学习（Reiforcement Learning)：只有间接（隐含）的y：y^hat。根据(X, y^hat, goodness)的三元组学习到真正的y。通常学习过程是序列发生的(online)。比如在线广告系统（customer, ad choice, ad click earning)，网络扑克（cards, startegy, winning amount）\n",
    "\n",
    "+ 根据得到数据的方式（Protocol）：\n",
    "    + 被动：batch（填鸭式，一次喂个够）、online\n",
    "    + 主动：Active Learning，目的是减少需要标记的数量（一种半监督的实现思路？）\n",
    "    \n",
    "+ 根据X的类型：\n",
    "    + Concrete Features: 领域知识，特征有很强的物理意义（客户信息，病人履历等）\n",
    "    + Raw Features: 比CF抽象，较少的物理意义，比如图像的像素矩阵，声音等。可以转化为CF（特征工程），比如得到（对称性，密度）的二元特征\n",
    "    + Abstract Features: 没有物理意义，比如推荐系统中的X：（userid, itemid）。也可以转化为CF，比如得到user factor 和 item factor\n",
    "    \n",
    "    现实的系统中往往综合运用上述三种特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Hoeffding Inequality\n",
    "\n",
    "   对于某个特定的h，和任意一个大小为N的数据D（iid自真实分布P(X, Y)），h的in-sample error和out-sample error满足下面的概率条件：\n",
    "\n",
    "$$ P(|E_{in}(h) - E_{out}(h)| > \\epsilon) <= 2 e^{-2 \\epsilon^2 N} $$\n",
    "\n",
    "左边的概率还有一个名字，即P(D is Bad Case of h)。如果h在D上的$E_{in}$和$E_{out}$相差非常大（$> \\epsilon$），那么说D是h的Bad Case。\n",
    "\n",
    "   在epsilon非常小、N足够大的情况下，我们可以说$E_{in}(h) = E_{out}(h)$是PAC的（Probalistic Approximate Correct）。如果此时正好$E_{in}(h)$足够小，那么h是一个好的学习结果（h = f也是PAC的）。\n",
    "\n",
    "   但是上述是一种理想情况，因为不能对所有的问题（f）都猜一个固定的h，这样感性上也会觉得不靠谱（即$E_{in}(h)$很可能会很大）。对于某个问题，我们往往需要通过某个模型从一个巨大（甚至无限）的hypothesis set（H）中选出一个g。这个选择的结果可能会选到任意一个h，所以要求D在整个H中都不是Bad Case。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Finite Bin Hoeffding\n",
    "\n",
    "假设原始数据分布（X，Y）的样本大小为2N，已知的样本（D）数为N。那么P(D is Bad Case in the H)的上界是多少？\n",
    "\n",
    "我们已有样本空间是所有可能的D，里面有一些子集是$B_1, B_2, ...$，分别是$H_1, H_2, ...$的Bad Case。可以想象的出，这些子集是完全互斥的，而是有覆盖交叠。如果我们假设所有子集之间完全互斥，那么求得的P（D is Bad Case) 的上界会过于宽松。但是精确的求子集和的面积比非常难，所以我们只考虑那些完全重叠的子集，把它们的子集归一化（只计数一次），从而尽可能缩小上界。\n",
    "\n",
    "两个不同的hopythesis的Bad Case什么时候完全一样（重叠）呢？只能是他们对X有一样的分类结果。那么如果我们知道了X的最大可能分类结果的个数M，我们就知道了在将完全重叠的子集归一后，剩下的子集的个数，其也等于M。\n",
    "\n",
    "由于M依赖于具体的h和X的个数2N，所以应该是$m_h(2N)$。根据加法原理，我们可以得出P(D is Bad Case in the H)\n",
    "\n",
    "$$  = P(\\exists h, |E_{in}(h) - E_{out}(h)| > \\epsilon) <= 2 m_H(2N) e^{-2 \\epsilon^2 N} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Infinite Bin Hoeffding\n",
    "\n",
    "但是显然X的大小不是D的两倍，甚至可能不是有限的。那么我们辛辛苦苦求得的上界就没有意义了（也会是无限大），那么D在H上是Bad Case的可能性没有理论保证。那么我们就以此认定问题不可学习嘛？\n",
    "\n",
    "并不是的，我们可以用$E_{in}^{-}(h)$近似$E_{out}(h)$。其中前者是h在一个validation dataset（$D^{-}$，也是i.i.d.于X）上的error。再经过一些概率变换（这里略去，因为笔者目前还没有参透-_-），最后得到：\n",
    "\n",
    "$$ P(\\exists h, |E_{in}(h) - E_{out}(h)| > \\epsilon) <= 4 m_H(2N) e^{-\\frac{1}{8} \\epsilon^2 N} $$\n",
    "\n",
    "其中$m_H(2N) <= 2^N$，如果我们能把其上界缩小到从指数函数缩小到$poly(N)$，那么问题的学习就变成（概率）可行。我们引入Bounding Function（B(N, k)），其中k是H的最小的Break Point，即H不能shatter k个（或多于）数据点。进过一番推导（略），我们有$m_H(2N) <= B(N, k) = \\sum \\limits_{0<=i<k} \\binom{N}{i} $。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV. VC Dimension\n",
    "\n",
    "为了方便说明物理意义，我们引入VC维（$d_{vc}$）。VC维是关于H的一个属性，即是H最大的non-break point，也即最小的break point（k） - 1；H在k个数据点上的分类结果个数始终小于$2^k$。\n",
    "\n",
    "经过一番推导（略），得到：\n",
    "$$ m_H(N) <= N^{d_{vc}}, s.t. N >= 2, d_{vc} >= 2$$\n",
    "即我们有了$poly(N) = N ^ {d_{vc}}$，如果我们选择VC维合适的H，那么问题学习终于变得可行了！\n",
    "\n",
    "\n",
    "为了更方便的说明物理意义，我们另$ \\delta = 4 m_H(2N) e^{-\\frac{1}{8} \\epsilon^2 N}$，然后我们有很大的概率$( >= 1 - \\delta)$，满足：\n",
    "\n",
    "$$ E_{out}(g) <= E_{in}(g) + \\sqrt{\\frac{8}{N}ln(\\frac{4(2N)^{d_{vc}}}{\\delta})} $$\n",
    "\n",
    "其中最后的根号下的部分叫做对模型复杂度的惩罚：$\\Omega(N, d_{vc}, \\delta)$)。把$E_{out}(g)$看作$d_{vc}$的单变量函数，得到后者不能太大，也不能太小，需要in the middle。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V. 对实际应用的指导作用\n",
    "\n",
    "+ $d_{vc} \\approx $ 模型参数的个数（#free parameters）\n",
    "\n",
    "可以看出VC维的大小一定程度上代表了模型的能力。\n",
    "\n",
    "+ N的最小值是多少？\n",
    "\n",
    "根据$ \\delta = 4 m_H(2N) e^{-\\frac{1}{8} \\epsilon^2 N}$，我们一般需要的$\\epsilon=0.1$、$\\delta=0.1$，那么$N \\approx 10000 d_{vc}$。但是实际应用中一般满足$N \\approx 10 d_{vc}$的话效果就会比较好。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOW TO\n",
    "先讲三种线性模型（hypothesis和相应的参数解法），即h(x)=Wx。然后讲了线性特征的非线性变换。但是作为基础，我们最开始聊一聊Error Function（Err(h(x), y)）。Err加上P(y|x)定义了ideal mini-target function（即f(x)），而f加上噪声定义了y:\n",
    "\n",
    "+ Err(h(x), y) + P(y|x) => f(x)\n",
    "+ f(x) + noise = y\n",
    "\n",
    "真实的数据分布（X，Y）中的Y都类似上面掺杂了噪声。我们可以直接去求P(y|x)（在分类问题上叫软分类），也可以只求f(x)（在分类问题上叫硬分类）。两者都需要定义求解合适的Err实现。\n",
    "\n",
    "+ 求P(y|x)的Err称为True Err。\n",
    "    + 二项分类：$E_{CE} = ln(1 + e^{-yh(x)})$\n",
    "    \n",
    "    这叫交叉熵(cross entropy)。怎么得出的呢？我们用sigmoid函数$\\theta(x) = \\frac{1}{1 + e^{-x}}$来建模P(y|x)，有\n",
    "    \n",
    "    $$P(y|x) =\n",
    "        \\begin{cases}\n",
    "        \\theta(h(x)) & \\text{y = +1}\\\\\n",
    "        1 - \\theta(h(x)) & \\text{y = -1}\\\\\n",
    "        \\end{cases}$$\n",
    "        \n",
    "    $$\\Downarrow 1 - \\theta(h(x)) = \\theta(-h(x)) $$\n",
    "    $$ P(y|x) = \\theta(yh(x)) = \\frac{1}{1 + e^{-yh(x)}} $$\n",
    "    \n",
    "    现在条件概率的猜想形式有了，但是伪真实（观察得到）的条件概率我们不知道，或者应该说太稀疏不可用，所以很难参数优化我们的猜想。所以我们考虑用必要条件代替充分条件，即如果我们猜想的P(y|x)跟真实的接近，则在他们之上的D的生成概率也会很接近，即都会很大。我们可以用猜想的P(y|x)形式化的表示出D的生成概率，取个反，我们需要这个取反后的概率尽可能小。这样一看，他就等效于我们需要尽可能小的error function。做一些变换，我们就得到了上面的point-wise error function $E_{CE}$。\n",
    "    \n",
    "    + 多项分类：$E_{Softmax} = - ln(\\frac{e^{h(x, y)}}{\\sum \\limits_{Y} e^{h(x, Y)}}) $\n",
    "    \n",
    "    这叫Softmax。怎么得出的呢？（todo）\n",
    "\n",
    "\n",
    "+ 求f(x)的Err有许多种，取决于我们需要什么样的f(x)，即通过后者反推前者。f(x)可以看做P(y|x)这个（条件）分布上的某个统计量，所以有很多选择。下面是一些f(x)的选择和其相应的Err：\n",
    "    + 概率最大的y：$E_{01} ＝ 1(sign(h(x)) != y) = 1(yh(x) <= 0) $\n",
    "    + y的期望：$E_{square} ＝ (h(x) - y)^2 = (yh(x) - 1)^2$\n",
    "    + y的概率中位数：$E_{abs} = |h(x) - y| = |yh(x) - 1|$\n",
    "    \n",
    "  但也可以先考虑定义Err，再看f(x)。这样的好处是可以实现Weighted Classification。拿二项分类举例，即对False Negative和False Positive两种错误在Err中赋予不同的权重。\n",
    "  \n",
    "下面我们可视化一下这几个error function："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEACAYAAAB8nvebAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d8f0a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "yhx = np.arange(-2,2.5,.02)\n",
    "err_ce = map(lambda x: math.log(1 + math.exp(-x)) / math.log(2), yhx)\n",
    "err_01 = map(lambda x: 1 if x <= 0 else 0, yhx)\n",
    "err_square = map(lambda x: (x - 1) * (x - 1), yhx)\n",
    "err_abs = map(lambda x: math.fabs(x - 1), yhx)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(yhx, err_ce, label='ce')\n",
    "ax.plot(yhx, err_01, label='01')\n",
    "ax.plot(yhx, err_square, label='square')\n",
    "ax.plot(yhx, err_abs, label='abs')\n",
    "\n",
    "# Now add the legend with some customizations.\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "\n",
    "# The frame is matplotlib.patches.Rectangle instance surrounding the legend.\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "\n",
    "# Set the fontsize\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，其它几个Err都是$E_{01}$的上界。注意，为了突出这一点，我们将$E_{CE}$的底由e换成了2.\n",
    "\n",
    "留下的问题是$E_{softmax}$怎么可视化（todo）？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. PLA 和 Pocket\n",
    "\n",
    "+ PLA\n",
    "    + error func: $ 0 $\n",
    "    + 权重更新公式：$W_{t+1} = W_t + Y_i * X_i, 其中(X_i, Y_i)是W_t分错的（任意、或按照一定顺序的某个）数据点 $\n",
    "    + 迭代停止（充要）条件：数据线性可分。证明的话需要得出：$\\frac{W_f^T}{||W_f||} * \\frac{W^t}{||W^t||} >= sqrt(t) * constant$，其中W_f是目标权重，$W^t$是迭代t次后的权重，t是迭代次数，constant是某个常数。因为$\\frac{W_f^T}{||W_f||} * \\frac{W^t}{||W^t||}$最大为1，所以t是有限的。\n",
    "    + 复杂度：\n",
    "    \n",
    "+ Pocket\n",
    "    在数据线性不可分的时候可以考虑用Pocket，其与PLA的不同：\n",
    "    + error func: $ E_{0/1} $\n",
    "    + 权重更新：只有PLA计算得到的$W_{t+1}的E_{0/1}比W_t小的时候$，才用前者替代后者\n",
    "    + 迭代停止条件：足够多的次数（经验值）\n",
    "    + 复杂度："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Linear Regression\n",
    "\n",
    "+ error func: $ E_{in}(h(x), y) = E_{square}(Wx, y) $\n",
    "+ 权重更新：没必要？\n",
    "+ 迭代停止条件：足够多的次数（经验值）\n",
    "+ 解析解：$ W_{LIN} = (X^T X)^{-1} X^T y $\n",
    "\n",
    "    注意，其中$(X^T X)^{-1} X^T $叫做X的伪逆，单独讲他是因为其在$X^T X$不可逆的时候（singular），还是有其他的方法可以算出。\n",
    "+ 复杂度："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Logistic Regression\n",
    "\n",
    "+ error func: $ E_{in}(h(x), y) = E_{CE}(Wx, y) $\n",
    "+ 权重更新：\n",
    "\n",
    "$$ W_{t+1} = W_t - \\eta \\nabla E_{in}(W_t) \n",
    "        = W_t - \\eta \\frac{1}{N} \\sum \\limits_{N} \\theta(-y_N W_t^T x_N) $$\n",
    "\n",
    "    这里$\\eta$叫做学习率（learning rate），控制梯度下降的速度。实践中一般取0.1。\n",
    "+ 迭代停止条件：足够多的次数（经验值）\n",
    "+ 解析解：无\n",
    "+ 复杂度："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了二元分类，还有多元分类。\n",
    "\n",
    "+ 通过二元分类实现的多(K)元分类\n",
    "    + One VS All（OVA）\n",
    "        + 复杂度：O(K * 二元分类的复杂度)\n",
    "        \n",
    "        优点：实现相对简单\n",
    "        \n",
    "        缺点：类别失衡\n",
    "        \n",
    "    + One VS One（OVO）\n",
    "        + 复杂度：O(K * K * 二元分类在最大的两个类别的数据集合上的复杂度）\n",
    "        \n",
    "        优点：不容易出现类别失衡、实际运行速度往往比OVA快\n",
    "        \n",
    "        缺点：需要的空间复杂度大（保存参数）\n",
    "        \n",
    "+ 直接多元建模的多元分类\n",
    "    + Softmax（todo）\n",
    "        + 复杂度：\n",
    "    + Multinomial Logistic Regression（todo）\n",
    "        + 复杂度："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV. Nolinear Transformation\n",
    "\n",
    "通过特征变换，将d个维度的原始特征，变换为Q（>= d）个维度（升维），达到了复用机器学习模型、同时提高模型VC维的目的（即模型的能力）。所以实践中要注意Q的合适选取。\n",
    "\n",
    "另外，通过上述变换，会得到$Q+d \\choose d$ 个维度，为什么?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOW TO BE BETTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. 刹车（Deal with Overfitting）\n",
    "\n",
    "可以把学习过程理解为开车，而VC维的大小理解为车速的大小。车的速度不是越快越好，要及时刹车，不然不安全。可以通过往Err上面加限制条件的方式达到限制（缩小）Hypothesis Set的目的，即降低VC维，刹车。加了限制条件的Err，可以通过拉格朗日乘子法变成一个加了Regularizer的新Err。举例说明：\n",
    "\n",
    "    \n",
    "常用的Regularizer有两种：\n",
    "    \n",
    "+ L1：$\\sum \\limits_{D} |W_D|$\n",
    "\n",
    "    优点是可以得到稀疏解。\n",
    "    \n",
    "+ L2(ridge)：$ \\sum \\limits_{D} W_D^2 $\n",
    "\n",
    "    优点是易于优化。\n",
    "    \n",
    "当然，也可以更具自己对当前问题的解的先验知识来选取特定的Regularizer，比如要让f满足对称性，可以选用symmetry regularizer：$ \\sum \\limits_{D} 1(D=odd) W_D^2 $ 或 $ \\sum \\limits_{D} 1(D=odd) |W_D| $。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. 监控仪表盘（Parameter Tuning）\n",
    " + 到目前为止我们需要手工指定的参数（超参数）有：\n",
    "  + 哪种模型（LRegression? LR? Pocket?）\n",
    "  + 多项式非线性变换中的Q多大?\n",
    "  + 梯度下降中的学习率$\\eta$多大\n",
    "  + 哪种Regularizer？\n",
    "  + Weighted Classification 中 False Negative 和 False Positive的权重比是？\n",
    "  + 多元分类到底是用OVA 还是 OVO 还是 Softmax？\n",
    "    等等\n",
    "    \n",
    "  这些参数（决策）的组合本身就是指数增长的（假设最后的组合数目为M），如果这些组合统统都在$E_{in}$上一较高下，那么就相当于拿了一个VC维度非常巨大（$\\sum \\limits_{M}|H_m|，|H_m|是h_m的Hypothesis Set大小$）的Hypothesis Set来进行优化，找出其中的g。这就相当于有了一个VC维度巨大的新的模型，这就不能保证问题是可以学习的了（参考WHY小节）。那么如果我们在一个保留的validation Dataset上验证Err（$E_{val}$），那么我们用来找g的Hypothesis Set的大小则是M。这样在M不是很大的时候，问题就是可以学习的。\n",
    "\n",
    "  但是我们去哪里找这个validation Dataset呢？方法便是从D中保留一块，这种方法叫做Cross Validation，具体分为：\n",
    "\n",
    " + Leave One Out(LOO)\n",
    " \n",
    "  策略：D的大小为N，则每次用N-1个数据训练，然后用剩下的1个数据计算$E_{val}$，枚举每个数据，最后求得$E_{val}$的均值就是的$E_{val}$：\n",
    "    \n",
    "   $$ E_{loo}(H_m) = \\frac{1}{N} \\sum \\limits_{N} E_{val}(H_m, n)\n",
    "                = \\frac{1}{N} \\sum \\limits_{N} E_{point-wise}(h_m(X_n), Y_n) $$\n",
    "            \n",
    "  其中$h_m$是$A_m$基于$D-X_n$的数据集，在$H_m$空间中找到的最优的h（这是一个训练过程）。注意，这里用了泛化的$E_{point-wise}$来代表所有可能的具体的point-wise的Err，因为后者也可能是一种超参数。\n",
    "    \n",
    "  那么最后要求的g就是：\n",
    "  $$ G = \\underset{H_m}{\\mathrm{argmin}}  E_{loo}(H_m) $$\n",
    "  $$ g = A_G (G, D) $$\n",
    "  其中G是g所在的Hypothesis Set，而$A_G$是跟其配套的算法。注意最后的g是在整个D上计算所得。可以证明（略）：$E_{loo}(H_m)$ ‘几乎’是$E_{out}(g_m)$的无偏估计(almost unbiased estimate）。而$E_{in}(H_m)$则不是。\n",
    "    \n",
    "  从上面可以看出LOO的复杂度为$O(N*\\sum单个模型的复杂度)$，当N很大时会非常高，实际应用非常有限。\n",
    "    \n",
    " + K-Fold\n",
    "\n",
    "  为了克服LOO复杂度高的缺点，我们才用V叠交叉验证代替之。其原理就是将D等分为K份，然后每次用一份作为Validation Dataset，枚举每一份。这样复杂度便降为$O(K*\\sum单个模型的复杂度)$。实际应用中K一般取5~10。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "III. Power of Three\n",
    "\n",
    "讲三个大有用处的小技巧。\n",
    "+ 奥卡姆剃刀\n",
    "\n",
    "    在所有合适的模型里面，选用尽可能简单的。\n",
    "    \n",
    "    \n",
    "+ 采样偏差\n",
    "\n",
    "    训练数据和测试数据可能通过不同的分布从真实分布中采样得到。\n",
    "    \n",
    "    \n",
    "+ 数据偷看\n",
    "\n",
    "    通过对训练数据进行挖掘，抽取出更好的特征（$E_{in}$小）。其实你人工抽取特征的过程也是一个Hypothesis Set增大的过程，这样你就无形中增加了模型的VC维，可能使得问题的学习变的不可行。\n",
    "    \n",
    "    但也不是一点也不能偷看，还是回到VC维度大小的选取问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自己的总结\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 判别式和生成式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    上面模型的Hypothesis，不管是直接建模P(y|x)，还是通过建模f来间接近似P(y|x)，都是对条件概率P(y|x)建模。这种模型是判别式的。\n",
    "\n",
    "    而也可以对全概率P(x, y)建模，这样的模型是生成式的。虽然我们的目的还是求的P(y|x)，但这可以很容易的通过贝叶斯定理实现：\n",
    "\n",
    "$$ P(y|x) = \\frac{P(x, y)}{P(x)}$$\n",
    "\n",
    "    由于P(x)是固定的，我们可以只求出针对不同y的P(x,y)，然后做一个归一化就行了。那P(x,y)到底该怎么建模呢？一半都是基于一些假设，比如我们可以假设：\n",
    "    \n",
    "$$ P(x,y) = P(y) * \\prod_D P(x_d | y)$$\n",
    "\n",
    "    这叫朴素贝叶斯假设。基于这种假设的模型常用于垃圾邮件分类、拼写提示等。\n",
    "    \n",
    "    生成式和判别式模型有许多可以对比的地方，也各有优缺点。但是这里只说一个：既然前者基于某个假设，那么他其实是强制把模型的Hypothesis Set固定在一个经过这个假设正则化后的空间中。如果这个Regularizer可以很好的描述D，那么模型就work了，而且需要的训练数据也较小（为什么？提示：VC维低了）。否则，她就可能效果很差。所以应用生成式模型之前一定要研究一下数据是否符合该模型的假设。\n",
    "\n",
    "    两种模型的详细比较准备单独一讲总结（todo）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
