{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 组合模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动机\n",
    "\n",
    "合适的组合模型可以实现提高模型能力（踩油门），又可以实现正则化（踩刹车），好像把两个矛盾的东西结合在了一起。\n",
    "\n",
    "组合模型合适的充分必要条件是各个g差异性大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在$g_t$已知的情况下，有三种组合方法：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ Uniform Blending\n",
    " + 二元分类\n",
    " \n",
    "  $$G(x) = sign(\\sum_T g_t(x))$$\n",
    "  \n",
    " + 多元分类\n",
    " \n",
    "  $$G(x) = arg\\max_K \\sum_T 1(g_t(x) = k)$$\n",
    " \n",
    " + 回归\n",
    " \n",
    "  $$G(x) = \\frac{1}{T} \\sum_T g_t(x)$$\n",
    " \n",
    " 可以证明，单个g的$E_{out}$的期望值比G的$E_{out}$大。\n",
    " \n",
    " \n",
    "+ Linear Blending\n",
    " + 二元分类\n",
    " \n",
    "  $$G(x) = \\hat{g}(\\Phi(x))$$\n",
    "  \n",
    " + 多元分类\n",
    " \n",
    "  $$G(x) = \\hat{g}(\\Phi(x))$$\n",
    "  \n",
    " + 回归\n",
    " \n",
    "  $$G(x) = \\hat{g}(\\Phi(x))$$\n",
    "   \n",
    " 算法：把D分为$D_{train}$和$D_{val}$，在前者上训练得到各个$g_t^{-}$，另$\\Phi^{-}(x) = (g_1^{-}(x),...,g_t^{-}(x))$，然后把$D_{val}$中的数据$(x_n, y_n)$变换为$(z_n = \\Phi^{-}(x_n), y_n)$。用变换后新数据在$D_{val}$上训练一个线性模型（超参数），得到$\\hat{g}$，其参数是$\\alpha$。\n",
    " \n",
    " 注意我们在G中采用的是$\\Phi(x) = (g_1(x),...,g_t(x))$，而不是$\\Phi^{-}$。g是在D上训练得到，而$g_{-}$只在$D_{train}$上训练得到。另外，从上面$\\alpha$的G的求解可以看出，各个$g_t$相当于对原数据做了一个特征变换：$x -> (g_1(x),...,g_t(x))$\n",
    " \n",
    " \n",
    "+ Conditional Blending (Stacking，又叫Any Blending)\n",
    "\n",
    " 在经过和Linear Blending一样的特征变换后，利用这些新的数据在$D_{val}$上训练一个非线性模型（超参数），得到$\\hat{g}$，然后G就变为：\n",
    " \n",
    " + 二元分类\n",
    " \n",
    "  $$G(x) = \\hat{g}(\\Phi(x))$$\n",
    "  \n",
    " + 多元分类\n",
    " \n",
    "  $$G(x) = \\hat{g}(\\Phi(x))$$\n",
    "  \n",
    " + 回归\n",
    " \n",
    "  $$G(x) = \\hat{g}(\\Phi(x))$$\n",
    " \n",
    " 之所以Linear Blending和Stacking都是在$D_{val}$进行组合模型的选择，原因和Validation的必要性是一个道理。其实Validation可以看做上述三种方法以外的另一种组合模型，那便是$G(x) = (arg\\min_{T} E_{val}(g_t)) (x)$。即最后的组合模型中只保留$E_{val}$最小的g。相应的，前三种方法也可以进行Cross Validation，以便最后得到的G的$E_{val}$更接近$E_{out}$。但是这就要求把D分成三份（为什么？）\n",
    " \n",
    " 可以看出，Linear Blending和Stacking都是一种两层学习（two-level learning）的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在$g_t$未知的情况下，我们必须先求出$g_t$，然后组合。也有三种方法：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ Bagging（属于Uniform Blending）\n",
    "\n",
    " 对D不断进行有放回采样，得到T个$D_t$，大小同D。然后在$D_t$上训练得到$g_t$。\n",
    " \n",
    " 应用场景：$g_t$对数据的变化比较敏感（容易过拟合）。这样因为$D_t$的不同得到的$g_t$会有较大差异性。\n",
    "\n",
    "+ Adaptive Boosting (简称AdaBoost，属于Linear Blending)\n",
    "\n",
    " 动机：迭代生成各个$g_t$（要求准确率大于0.5），其中$E_{in}^{t+1}$比$E_{in}^{t}$更关注$g_t$分错的数据。这里“更关注”的思想主要通过为每个数据点赋予特定的权重（$u_i$，可以看作一个copy操作数，但是可以泛化为分数）实现。类似的做法可以用来实现Weighted Classification。注意，赋予权重使得D的分布发生变化，相应的g的也会发生变化，随之而来的也有求g的算法的变化。举例来说：\n",
    " \n",
    "  + Logistic Regression (SGD)\n",
    "   + error func: $ E_{weighted-element-wise}(h(x_i), y) = u_i E_{CE}(h(x_i), y) $\n",
    "   + 权重更新：\n",
    "\n",
    "     $$ W_{t+1} = W_t - \\eta \\nabla E_{in}(W_t) $$\n",
    "    \n",
    "     $$    = W_t - \\eta \\theta(-y_n W_t^T x_n) $$\n",
    "    \n",
    "     其中$(x_n, y_n)$按照$norm(u_1,...,u_n)$的分布采样于D。\n",
    "     \n",
    "   +  SVM（todo）\n",
    " \n",
    " 算法：（这里只说明分类问题，回归问题目前没有参透）\n",
    " \n",
    "  1. 初始化$u = (\\frac{1}{N},...)$，其中N是D的大小。下面迭代O(logN)次。\n",
    " \n",
    "  2. 每一次迭代t，计算$g_t$，另$e_t$是其在权重$u_t$的D上的01错误率，简称权重错误率。同时更新权重：\n",
    "      \n",
    "   $$u_{t+1} =\n",
    "\\begin{cases}\n",
    "u_t \\sqrt[2]{\\frac{1-e_t}{e_t}} & \\text{y != g_t(x)}\\\\\n",
    "\\frac{u_t} {\\sqrt[2]{\\frac{1-e_t}{e_t}}} & \\text{y = g_t(x)}\\\\\n",
    "\\end{cases}$$\n",
    "   \n",
    "   可以看出，在$e_t$大于0.5的限制下，分类错误数据点的权重在下一轮会被放大。\n",
    "   \n",
    "  3. 计算每个$g_t$在Linear Blending中的权重系数$\\alpha_t$：\n",
    "  \n",
    "       $$\\alpha_t = ln(\\frac{1-e_t}{e_t}) $$\n",
    "  \n",
    "   这是一个启发式的策略，即权重错误率越低，$g_t$的权重越大。\n",
    "   \n",
    "  最后得到$G(x) = sign(\\sum_T \\alpha_t g_t(x)) $（拿二元分类举例）。\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  \n",
    "  + Decision Tree（对应于Stacking）\n",
    "  \n",
    "    两种公式：\n",
    "    \n",
    "    DT模型的特点是可解释强，但是超参数众多。\n",
    "    \n",
    "    + C&RT\n",
    "    \n",
    "      算法：\n",
    "      \n",
    "        + Terminal Criteria: all x are same or all y are same\n",
    "        + Base Hypothesis: Constant\n",
    "        + Branching: Decision Stump Gini\n",
    "        + Branch Num: 2\n",
    "  \n",
    "      C&AT的优点非常多：\n",
    "        \n",
    "        + 很好的处理缺失值\n",
    "        + 既能分类又能回归\n",
    "        + 快速的非线性模型\n",
    "        \n",
    "      C&RT有着单分类器里面最多的优点，但是其缺点是对数据非常敏感，容易过拟合。所以我们想到限制他的表达能力（VC维）。C&RT的VC维度近似等于她的叶子数。\n",
    "      \n",
    "    + Random Forest（C&RT + Bagging）\n",
    "    \n",
    "      算法：\n",
    "      \n",
    "      RF的动机：C&RT有着单分类器里面最多的优点，但是其缺点是对数据非常敏感（RF中的C&RT都是full-growth的，也就是说$E_{in}=0$）。而这正好是Bagging组合算法的菜。前边提到过Bagging只有在Base Hypothesis差异性比较大的时候效果才比较好。同时组合模型有同时有刹车（正则化，其实就是使得G的边界比较平滑）的作用。所以自然而然的结合C&AT和Bagging，得到了RF模型。\n",
    "      \n",
    "      + Out-Of-Bag Estimate（OOB，RF进行交叉验证的方法，同LOO，是一种self-validation）\n",
    "        \n",
    "        RF里面的随机行很多，比如Randomly Projected Subspace的大小、其坐标向量里面非零元素的数目等等。也就是超参数非常多，所以需要做好模型选择的工作。类比于针对单模型的LOOCV（用来躲避显示定义$D_{val}$），我们可以利用OOB做RF的CV，从而在超参数空间做选择。OOB是On-The-Fly的，实际应用中非常高效。\n",
    "  \n",
    "      + Feature Selection（RF的一个非常好的应用）\n",
    "  \n",
    "        + 利用Linear Model\n",
    "        \n",
    "          $$importance(i) = |w_i| $$\n",
    "\n",
    "        + Permutation Test\n",
    "          必要性条件当充分性条件\n",
    "          \n",
    "          $$importance(i) = performance(D) - performance(D_p) $$\n",
    "          $$              = -E_{oob}(G) + E_{oob}(G^p) $$\n",
    "          $$              \\approx -E_{oob}(G) + E_{oob}^p(G) $$\n",
    "           \n",
    "      RF的缺点是其中有大量的随机性，所以需要合适（1000+？）的C&RT来确保稳定性。另外RF与下面讲的GBDT之间的区别是，前者的DT必须为Strong的，即RF中的C&RT都是full-growth的，也就是说$E_{in}=0$。\n",
    "\n",
    "    + Gradient Boosting Decision Tree (C&AR + AdaBoost)\n",
    "    \n",
    "      GBDT的动机：既然Bagging可以组合C&AT，那么AdaBoost为什么不能？（没准效果还更好，但是VC维？）\n",
    "      \n",
    "      由于AdaBoost可以套用不同的base hypothesis，如果后者为C&RT，则叫做AdaBoost Decision Tree。其他则统统叫GBDT。\n",
    "      \n",
    "      AdaBoost可以从（函数）优化的角度来诠释。这里每一个$g_t$，及其在G中的权重系数$\\alpha_t$都可以看作一次梯度下降的结果。原始的优化问题的Err是指数Err：$E_{exp}(y, h(x)) = e^{-yh(x)}$，她也是$E_{01}$的上界（见下图）。既然将AdaBoost整个抽象为了一个最优化问题，所以这里的Err便可以随便换了。比如可以换成$E_{square}$，这样就可以解决回归问题了。\n",
    "      \n",
    "      GBDT与上面讲的RF之间的区别是，前者的DT必须为weak的，即$E_{in}(g_t) > ０$。因为$\\alpha_t$在$$E_{in}(g_t) = ０$时为无穷大，造成$g_t$一家独大，GBDT的组合性就没了意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEACAYAAAB8nvebAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqiAJ3kIIUQVJ8BZCiCro/wGcXmRkOAMs3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107b78190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "yhx = np.arange(-2,2.5,.02)\n",
    "err_exp = map(lambda x: math.exp(-x), yhx)\n",
    "err_01 = map(lambda x: 1 if x <= 0 else 0, yhx)\n",
    "err_square = map(lambda x: (x - 1) * (x - 1), yhx)\n",
    "err_abs = map(lambda x: math.fabs(x - 1), yhx)\n",
    "err_ce = map(lambda x: math.log(1 + math.exp(-x)) / math.log(2), yhx)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(yhx, err_exp, label='exp')\n",
    "ax.plot(yhx, err_01, label='01')\n",
    "ax.plot(yhx, err_square, label='square')\n",
    "ax.plot(yhx, err_abs, label='abs')\n",
    "ax.plot(yhx, err_ce, label='ce')\n",
    "\n",
    "# Now add the legend with some customizations.\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "\n",
    "# The frame is matplotlib.patches.Rectangle instance surrounding the legend.\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "\n",
    "# Set the fontsize\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然Boost模型都许多的参数（超参数、启发式策略）的选择，在实践中就需要一个稳定和有效的代码实现。经过众多的验证（kaggle比赛等）和推荐，XGBoost脱颖而出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特点：\n",
    "------\n",
    "\n",
    "[引用](http://cos.name/2015/03/xgboost/)\n",
    "\n",
    "  只要能够求出目标函数的梯度和Hessian矩阵，用户就可以自定义训练模型时的目标函数。[demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py)\n",
    "  \n",
    "  允许用户在交叉验证时自定义误差衡量方法，例如回归中使用RMSE还是RMSLE，分类中使用AUC，分类错误率或是F1-score。甚至是在希格斯子比赛中的“奇葩”衡量标准AMS。[demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/cross_validation.py)\n",
    "  \n",
    "  交叉验证时可以返回模型在每一折作为预测集时的预测结果，方便构建ensemble模型。demo（同上）\n",
    "  \n",
    "  允许用户先迭代1000次，查看此时模型的预测效果，然后继续迭代1000次，最后模型等价于一次性迭代2000次。[demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/boost_from_prediction.py)\n",
    "  \n",
    "  可以知道每棵树将样本分类到哪片叶子上，[facebook介绍过如何利用这个信息提高模型的表现](http://quinonero.net/Publications/predicting-clicks-facebook.pdf)。[demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/predict_leaf_indices.py)\n",
    "  \n",
    "  可以计算变量重要性并画出树状图。[API](http://xgboost.readthedocs.org/en/latest/python/python_intro.html#plotting)\n",
    "  \n",
    "  可以选择使用线性模型替代树模型，从而得到带L1+L2惩罚的线性回归或者logistic回归。[demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/generalized_linear_model.py)\n",
    "  \n",
    "  -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原理\n",
    "\n",
    "[原理](https://xgboost.readthedocs.org/en/latest/model.html)\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实操（xgboost + sklearn）\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 控制过拟合的参数调整 [引用](https://xgboost.readthedocs.org/en/latest/param_tuning.html)\n",
    "\n",
    "  + [Understanding Bias-Variance Tradeoff](https://xgboost.readthedocs.org/en/latest/parameter.html)\n",
    "\n",
    "  + Control Overfitting\n",
    "   When you observe high training accuracy, but low tests accuracy, it is likely that you encounter overfitting problem.\n",
    "\n",
    "   There are in general two ways that you can control overfitting in xgboost\n",
    "\n",
    "   + The first way is to directly control model complexity\n",
    "     + This include max_depth, min_child_weight and gamma\n",
    "   + The second way is to add randomness to make training robust to noise\n",
    "     + This include subsample, colsample_bytree\n",
    "     + You can also reduce stepsize eta, but needs to remember to increase num_round when you do so.\n",
    "  \n",
    "  + Handle Imbalanced Dataset\n",
    "   For common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of xgboost model, and there are two ways to improve it.\n",
    "\n",
    "   + If you care only about the ranking order (AUC) of your prediction\n",
    "     + Balance the positive and negative weights, via scale_pos_weight\n",
    "     + Use AUC for evaluation\n",
    "   + If you care about predicting the right probability\n",
    "     + In such a case, you cannot re-balance the dataset\n",
    "     + In such a case, set parameter max_delta_step to a finite number (say 1) will help convergence\n",
    "    \n",
    "+ 和sklearn的整合\n",
    "\n",
    "  因为xgboost有一些训练参数（如early_stopping_rounds，有加速的功能）不能很好的整合进sklearn，所以我们采用的方法是：把sklearn里面一些并行函数抠出来利用。（之前想的是先用GridSearchCV选参数，然后利用选得的参数再训练一遍xgboost，这次利用Early Stop得到最佳的estimator个数。但是这样加速效果不明显。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   1 jobs       | elapsed:    0.3s\n",
      "[Parallel(n_jobs=2)]: Done   7 out of   9 | elapsed:    0.7s remaining:    0.2s\n",
      "[Parallel(n_jobs=2)]: Done   9 out of   9 | elapsed:    0.8s finished\n",
      "Will train until cv error hasn't decreased in 3 rounds.\n",
      "Will train until cv error hasn't decreased in 3 rounds.\n",
      "[0]\tcv-test-error:0.0144326666667+0.000946453849318\tcv-train-error:0.0144326666667+0.000472983673667\n",
      "[0]\tcv-test-error:0.0144326666667+0.000946453849318\tcv-train-error:0.0144326666667+0.000472983673667\n",
      "[1]\tcv-test-error:0.00122833333333+0.000217317484085\tcv-train-error:0.00122866666667+0.000108423039782\n",
      "[1]\tcv-test-error:0.00122833333333+0.000217317484085\tcv-train-error:0.00122866666667+0.000108423039782\n",
      "[2]\tcv-test-error:0.001996+0.00192986286214\tcv-train-error:0.00130533333333+0.00113359526385\n",
      "[2]\tcv-test-error:0.001996+0.00192986286214\tcv-train-error:0.00130533333333+0.00113359526385\n",
      "[3]\tcv-test-error:0.000921333333333+0.000651481047733\tcv-train-error:0.000768+0.000543058007951\n",
      "[3]\tcv-test-error:0.000921333333333+0.000651481047733\tcv-train-error:0.000768+0.000543058007951\n",
      "[4]\tcv-test-error:0.000921333333333+0.000651481047733\tcv-train-error:0.000768+0.000543058007951\n",
      "[4]\tcv-test-error:0.000921333333333+0.000651481047733\tcv-train-error:0.000768+0.000543058007951\n",
      "[5]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[5]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[6]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[6]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[7]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[7]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[8]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[8]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "Stopping. Best iteration:\n",
      "[5] cv-mean:0.0\tcv-std:0.0\n",
      "Stopping. Best iteration:\n",
      "[5] cv-mean:0.0\tcv-std:0.0\n",
      "Will train until cv error hasn't decreased in 3 rounds.\n",
      "Will train until cv error hasn't decreased in 3 rounds.\n",
      "[0]\tcv-test-error:0.0144326666667+0.000946453849318\tcv-train-error:0.0144326666667+0.000472983673667\n",
      "[0]\tcv-test-error:0.00122833333333+0.000217317484085\tcv-train-error:0.00122866666667+0.000108423039782\n",
      "[1]\tcv-test-error:0.00122833333333+0.000217317484085\tcv-train-error:0.00122866666667+0.000108423039782\n",
      "[1]\tcv-test-error:0.000921333333333+0.000651481047733\tcv-train-error:0.000768+0.000543058007951\n",
      "[2]\tcv-test-error:0.001996+0.00192986286214\tcv-train-error:0.00130533333333+0.00113359526385\n",
      "[2]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[3]\tcv-test-error:0.000921333333333+0.000651481047733\tcv-train-error:0.000768+0.000543058007951\n",
      "[3]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[4]\tcv-test-error:0.000921333333333+0.000651481047733\tcv-train-error:0.000768+0.000543058007951\n",
      "[4]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[5]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[5]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[6]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "Stopping. Best iteration:\n",
      "[2] cv-mean:0.0\tcv-std:0.0\n",
      "Will train until cv error hasn't decreased in 3 rounds.\n",
      "[7]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[0]\tcv-test-error:0.00122833333333+0.000217317484085\tcv-train-error:0.00122866666667+0.000108423039782\n",
      "[8]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[1]\tcv-test-error:0.000921333333333+0.000651481047733\tcv-train-error:0.000768+0.000543058007951\n",
      "Stopping. Best iteration:\n",
      "[5] cv-mean:0.0\tcv-std:0.0\n",
      "[2]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "Will train until cv error hasn't decreased in 3 rounds.\n",
      "[0]\tcv-test-error:0.00122833333333+0.000217317484085\tcv-train-error:0.00122866666667+0.000108423039782\n",
      "[3]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[1]\tcv-test-error:0.000921333333333+0.000651481047733\tcv-train-error:0.000768+0.000543058007951\n",
      "[4]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[2]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[5]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[3]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "Stopping. Best iteration:\n",
      "[2] cv-mean:0.0\tcv-std:0.0\n",
      "Will train until cv error hasn't decreased in 3 rounds.\n",
      "[4]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[0]\tcv-test-error:0.00122833333333+0.000217317484085\tcv-train-error:0.00122866666667+0.000108423039782\n",
      "[5]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[1]\tcv-test-error:0.000921333333333+0.000651481047733\tcv-train-error:0.000768+0.000543058007951\n",
      "Stopping. Best iteration:\n",
      "[2] cv-mean:0.0\tcv-std:0.0\n",
      "[2]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "Will train until cv error hasn't decreased in 3 rounds.\n",
      "[0]\tcv-test-error:0.00122833333333+0.000217317484085\tcv-train-error:0.00122866666667+0.000108423039782\n",
      "[3]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[1]\tcv-test-error:0.000921333333333+0.000651481047733\tcv-train-error:0.000768+0.000543058007951\n",
      "[4]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[2]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[5]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[3]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "Stopping. Best iteration:\n",
      "[2] cv-mean:0.0\tcv-std:0.0\n",
      "Will train until cv error hasn't decreased in 3 rounds.\n",
      "[4]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[0]\tcv-test-error:0.00122833333333+0.000217317484085\tcv-train-error:0.00122866666667+0.000108423039782\n",
      "[5]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[1]\tcv-test-error:0.000921333333333+0.000651481047733\tcv-train-error:0.000768+0.000543058007951\n",
      "Stopping. Best iteration:\n",
      "[2] cv-mean:0.0\tcv-std:0.0\n",
      "[2]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[3]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[4]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "[5]\tcv-test-error:0.0+0.0\tcv-train-error:0.0+0.0\n",
      "Stopping. Best iteration:\n",
      "[2] cv-mean:0.0\tcv-std:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_param\n",
      "{'objective': 'binary:logistic', 'num_boost_round': 10, 'eta': 1, 'max_depth': 3, 'silent': 1}\n",
      "test accuracy: \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "import copy\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris, load_digits, load_boston\n",
    "import operator as op\n",
    "import sys\n",
    "\n",
    "iris = load_iris()\n",
    "y = iris['target']\n",
    "X = iris['data']\n",
    "idx = y != 1\n",
    "y = y[idx]\n",
    "y[y == 2] = 1\n",
    "X = X[idx,:]\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "dtrain = xgb.DMatrix('/Users/DY/Desktop/xgboost/demo/data/agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('/Users/DY/Desktop/xgboost/demo/data/agaricus.txt.test')\n",
    "\n",
    "\n",
    "estimator_params_grid = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'eta': [1], 'silent': [1],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'num_boost_round': [10,20,30],\n",
    "}\n",
    "fit_params={'early_stopping_rounds':3, 'verbose_eval':True}\n",
    "\n",
    "def _fit_and_score_(cross_validator, estimator_params, train_file_path, **kwargs):\n",
    "    dtrain = xgb.DMatrix(train_file_path)\n",
    "    bst = cross_validator(estimator_params, dtrain, **kwargs)\n",
    "    return (bst['test-error-mean'].min(), estimator_params) \n",
    "\n",
    "def xgb_GridSearchCV(estimator_params_grid, train_file_path, fit_params={}):\n",
    "\n",
    "    parameter_iter = ParameterGrid(estimator_params_grid)\n",
    "\n",
    "    out = Parallel(\n",
    "        n_jobs=2, verbose=1,\n",
    "        pre_dispatch='2*n_jobs'\n",
    "    )(\n",
    "        delayed(_fit_and_score_)(xgb.cv, parameters, train_file_path, **fit_params)\n",
    "        for parameters in parameter_iter)\n",
    "\n",
    "    out = sorted(out, key=op.itemgetter(0))\n",
    "\n",
    "    best_param = out[0][1]\n",
    "\n",
    "    print 'best_param'\n",
    "    print best_param\n",
    "\n",
    "    best_model = xgb.train(best_param, dtrain)\n",
    "    return best_model\n",
    "\n",
    "\n",
    "labels = dtest.get_label()\n",
    "best_model = xgb_GridSearchCV(estimator_params_grid, '/Users/DY/Desktop/xgboost/demo/data/agaricus.txt.train', fit_params)\n",
    "preds = best_model.predict(dtest)\n",
    "print 'test accuracy: '\n",
    "print float(sum(1 if labels[i] == (preds[i] > 0.5) else 0 for i in xrange(len(labels)))) / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
