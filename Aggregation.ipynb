{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 组合模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动机\n",
    "\n",
    "合适的组合模型可以实现提高模型能力（踩油门），又可以实现正则化（踩刹车），好像把两个矛盾的东西结合在了一起。\n",
    "\n",
    "组合模型合适的充分必要条件是各个g差异性大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在$g_t$已知的情况下，有三种组合方法：\n",
    "\n",
    "+ Uniform Blending\n",
    " + 二元分类\n",
    " \n",
    "  $$G(x) = sign(\\sum_T g_t(x))$$\n",
    "  \n",
    " + 多元分类\n",
    " \n",
    "  $$G(x) = arg\\max_K \\sum_T 1(g_t(x) = k)$$\n",
    " \n",
    " + 回归\n",
    " \n",
    "  $$G(x) = \\frac{1}{T} \\sum_T g_t(x)$$\n",
    " \n",
    " 可以证明，单个g的$E_{out}$的期望值比G的$E_{out}$大。\n",
    " \n",
    " \n",
    "+ Linear Blending\n",
    " + 二元分类\n",
    " \n",
    "  $$G(x) = \\hat{g}(\\Phi(x))$$\n",
    "  \n",
    " + 多元分类\n",
    " \n",
    "  $$G(x) = \\hat{g}(\\Phi(x))$$\n",
    "  \n",
    " + 回归\n",
    " \n",
    "  $$G(x) = \\hat{g}(\\Phi(x))$$\n",
    "   \n",
    " 算法：把D分为$D_{train}$和$D_{val}$，在前者上训练得到各个$g_t^{-}$，另$\\Phi^{-}(x) = (g_1^{-}(x),...,g_t^{-}(x))$，然后把$D_{val}$中的数据$(x_n, y_n)$变换为$(z_n = \\Phi^{-}(x_n), y_n)$。用变换后新数据在$D_{val}$上训练一个线性模型（超参数），得到$\\hat{g}$，其参数是$\\alpha$。\n",
    " \n",
    " 注意我们在G中采用的是$\\Phi(x) = (g_1(x),...,g_t(x))$，而不是$\\Phi^{-}$。g是在D上训练得到，而$g_{-}$只在$D_{train}$上训练得到。另外，从上面$\\alpha$的G的求解可以看出，各个$g_t$相当于对原数据做了一个特征变换：$x -> (g_1(x),...,g_t(x))$\n",
    " \n",
    " \n",
    "+ Conditional Blending (Stacking，又叫Any Blending)\n",
    "\n",
    " 在经过和Linear Blending一样的特征变换后，利用这些新的数据在$D_{val}$上训练一个非线性模型（超参数），得到$\\hat{g}$，然后G就变为：\n",
    " \n",
    " + 二元分类\n",
    " \n",
    "  $$G(x) = \\hat{g}(\\Phi(x))$$\n",
    "  \n",
    " + 多元分类\n",
    " \n",
    "  $$G(x) = \\hat{g}(\\Phi(x))$$\n",
    "  \n",
    " + 回归\n",
    " \n",
    "  $$G(x) = \\hat{g}(\\Phi(x))$$\n",
    " \n",
    " 之所以Linear Blending和Stacking都是在$D_{val}$进行组合模型的选择，原因和Validation的必要性是一个道理。其实Validation可以看做上述三种方法以外的另一种组合模型，那便是$G(x) = (arg\\min_{T} E_{val}(g_t)) (x)$。即最后的组合模型中只保留$E_{val}$最小的g。相应的，前三种方法也可以进行Cross Validation，以便最后得到的G的$E_{val}$更接近$E_{out}$。那么就可以在多个G之间做选择。（有没有必要？因为组合的时候可以通过权重控制各个g的作用程度）\n",
    " \n",
    " 可以看出，Linear Blending和Stacking都是一种两层学习（two-level learning）的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在$g_t$未知的情况下，我们必须先求出$g_t$，然后组合。也有三种方法：\n",
    "\n",
    "+ Bagging（属于Uniform Blending）\n",
    "\n",
    " 对D不断进行有放回采样，得到T个$D_t$，大小同D。然后在$D_t$上训练得到$g_t$。\n",
    " \n",
    " 应用场景：$g_t$对数据的变化比较敏感。这样因为$D_t$的不同得到的$g_t$会有较大差异性。\n",
    "\n",
    "+ Adaptive Boosting (简称AdaBoost，属于Linear Blending)\n",
    "\n",
    " 动机：迭代生成各个$g_t$（要求准确率大于0.5），其中$E_{in}^{t+1}$比$E_{in}^{t}$更关注$g_t$分错的数据。这里“更关注”的思想主要通过为每个数据点赋予特定的权重（$u_i$，可以看作一个copy操作数，但是可以泛化为分数）实现。类似的做法可以用来实现Weighted Classification。注意，赋予权重使得D的分布发生变化，相应的g的也会发生变化，随之而来的也有求g的算法的变化。举例来说：\n",
    " \n",
    "  + Logistic Regression (SGD)\n",
    "   + error func: $ E_{weighted-element-wise}(h(x_i), y) = u_i E_{CE}(h(x_i), y) $\n",
    "   + 权重更新：\n",
    "\n",
    "     $$ W_{t+1} = W_t - \\eta \\nabla E_{in}(W_t) $$\n",
    "    \n",
    "     $$    = W_t - \\eta \\theta(-y_n W_t^T x_n) $$\n",
    "    \n",
    "     其中$(x_n, y_n)$按照$norm(u_1,...,u_n)$的分布采样于D。\n",
    "     \n",
    "   +  SVM（todo）\n",
    " \n",
    " 算法：（这里只说明分类问题，回归问题目前没有参透）\n",
    " \n",
    "  1. 初始化$u = (\\frac{1}{N},...)$，其中N是D的大小。下面迭代O(logN)次。\n",
    " \n",
    "  2. 每一次迭代t，计算$g_t$，另$e_t$是其在权重$u_t$的D上的01错误率，简称权重错误率。同时更新权重：\n",
    "      \n",
    "   $$u_{t+1} =\n",
    "\\begin{cases}\n",
    "u_t \\sqrt[2]{\\frac{1-e_t}{e_t}} & \\text{y != g_t(x)}\\\\\n",
    "\\frac{u_t} {\\sqrt[2]{\\frac{1-e_t}{e_t}}} & \\text{y = g_t(x)}\\\\\n",
    "\\end{cases}$$\n",
    "   \n",
    "   可以看出，在$e_t$大于0.5的限制下，分类错误数据点的权重在下一轮会被放大。\n",
    "   \n",
    "  3. 计算每个$g_t$在Linear Blending中的权重系数$\\alpha_t$：\n",
    "  \n",
    "       $$\\alpha_t = ln(\\frac{1-e_t}{e_t}) $$\n",
    "  \n",
    "   这是一个启发式的策略，即权重错误率越低，$g_t$的权重越大。\n",
    "   \n",
    "  最后得到$G(x) = sign(\\sum_T \\alpha_t g_t(x)) $（拿二元分类举例）。\n",
    "  \n",
    "  \n",
    "+ Decision Tree（对应于Stacking）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
